{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "sys.path.append('../../lib/')\n",
    "sys.path.append('../../lib/counter_gen_lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import *\n",
    "import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/workspace/ceph_data/argument-undermining/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jo_df =  pd.read_pickle(data_path + \"jo_data/gen_data.pickle\")\n",
    "\n",
    "training_df = jo_df[jo_df.split =='train']\n",
    "valid_df = jo_df[jo_df.split =='val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = utility.clean_df(training_df)\n",
    "valid_df    = utility.clean_df(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jo_training = utility.prepare_data_for_training({'train': training_df, 'valid': valid_df[0:5000]},\n",
    "                                                full_counter=False, context='title+post', \n",
    "                                                post_clm='post', \n",
    "                                                comment_clm='comment_sents', \n",
    "                                                attacks_clm='premise_counter_premise_pairs', max_sens=20, baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(jo_training, open(data_path + 'gpt_data/jo_data.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_LIB_PATH='/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with weak premises annotated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Running process -1\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Arguments: Namespace(baseline=False, build_instance_version='v2', dataset_cache='./jo_data_cache', dataset_path='/workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json', device='cuda', eval_before_start=False, fp16='', gradient_accumulation_steps=8, lm_coef=1.0, local_rank=-1, log_dir='/workspace/ceph_data/argument-undermining/models/gen_models/', lr=6.25e-05, max_history=2, max_norm=1.0, mc_coef=1.0, model_checkpoint='gpt2', n_epochs=4, num_candidates=2, output_model_checkpoint='my_approach_v2_4_epochs', personality_permutations=1, premise_extra=False, train_batch_size=1, valid_batch_size=1)\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Prepare tokenizer, pretrained model and optimizer.\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <argument> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <premise> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <counter> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning ['<argument>', '<premise>', '<counter>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Prepare datasets\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/utils.py:Load tokenized dataset from cache at ./jo_data_cache_GPT2Tokenizer\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build inputs and labels\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:using premise extra: False\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build instance version: v2\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Skipped 26278 cases out of 90426 in the train dataset!!\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:In 27158 cases, premise wasn't found in the post\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Skipped 2404 cases out of 8750 in the valid dataset!!\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:In 30166 cases, premise wasn't found in the post\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Pad inputs and convert to Tensor\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build train and validation dataloaders\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Train dataset (Batch, Candidates, Seq length): torch.Size([64148, 2, 509])\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Valid dataset (Batch, Candidates, Seq length): torch.Size([6346, 2, 509])\n",
      "/usr/local/lib/python3.6/dist-packages/ignite/handlers/checkpoint.py:708: UserWarning: Argument save_interval is deprecated and should be None. This argument will be removed in 0.5.0.Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n",
      "INFO:ignite.engine.engine.Engine:Engine run starting with max_epochs=4.\n",
      "Epoch [1/4]: [7/64148]   0%|                           , loss=14 [00:01<4:39:26]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Epoch [1/4]: [63790/64148]  99%|###################8, loss=0.422 [4:50:53<01:37]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 $sys.executable /workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json \\\n",
    "                    --model_checkpoint gpt2 \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --output_model_checkpoint my_approach_v2_4_epochs \\\n",
    "                    --dataset_cache ./jo_data_cache \\\n",
    "                    --build_instance_version 'v2' \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with weak premises annotated, and with special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Running process -1\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Arguments: Namespace(baseline=False, build_instance_version='v3', dataset_cache='./jo_data_cache', dataset_path='/workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json', device='cuda', eval_before_start=False, fp16='', gradient_accumulation_steps=8, lm_coef=1.0, local_rank=-1, log_dir='/workspace/ceph_data/argument-undermining/models/gen_models/', lr=6.25e-05, max_history=2, max_norm=1.0, mc_coef=1.0, model_checkpoint='gpt2', n_epochs=4, num_candidates=2, output_model_checkpoint='my_approach_v3_4_epochs', personality_permutations=1, premise_extra=False, train_batch_size=1, valid_batch_size=1)\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Prepare tokenizer, pretrained model and optimizer.\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <argument> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <premise> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <counter> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning ['<argument>', '<premise>', '<counter>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Prepare datasets\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/utils.py:Load tokenized dataset from cache at ./jo_data_cache_GPT2Tokenizer\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build inputs and labels\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:using premise extra: False\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build instance version: v3\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Skipped 26278 cases out of 90426 in the train dataset!!\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:In 27158 cases, premise wasn't found in the post\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Skipped 2404 cases out of 8750 in the valid dataset!!\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:In 30166 cases, premise wasn't found in the post\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Pad inputs and convert to Tensor\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build train and validation dataloaders\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Train dataset (Batch, Candidates, Seq length): torch.Size([64148, 2, 511])\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Valid dataset (Batch, Candidates, Seq length): torch.Size([6346, 2, 511])\n",
      "/usr/local/lib/python3.6/dist-packages/ignite/handlers/checkpoint.py:708: UserWarning: Argument save_interval is deprecated and should be None. This argument will be removed in 0.5.0.Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n",
      "INFO:ignite.engine.engine.Engine:Engine run starting with max_epochs=4.\n",
      "Epoch [1/4]: [7/64148]   0%|                         , loss=13.7 [00:01<4:43:11]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Epoch [1/4]: [4569/64148]   7%|#5                    , loss=0.46 [20:47<4:34:54]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 $sys.executable /workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json \\\n",
    "                    --model_checkpoint gpt2 \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --output_model_checkpoint my_approach_v3_4_epochs \\\n",
    "                    --dataset_cache ./jo_data_cache \\\n",
    "                    --build_instance_version 'v3' \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Running process -1\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Arguments: Namespace(baseline=True, build_instance_version=None, dataset_cache='./jo_data_cache', dataset_path='/workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json', device='cuda', eval_before_start=False, fp16='', gradient_accumulation_steps=8, lm_coef=1.0, local_rank=-1, log_dir='/workspace/ceph_data/argument-undermining/models/gen_models/', lr=6.25e-05, max_history=2, max_norm=1.0, mc_coef=1.0, model_checkpoint='gpt2', n_epochs=4, num_candidates=2, output_model_checkpoint='jo_baseline_gpt2', personality_permutations=1, premise_extra=False, train_batch_size=1, valid_batch_size=1)\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Prepare tokenizer, pretrained model and optimizer.\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <argument> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <premise> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <counter> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning ['<argument>', '<premise>', '<counter>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Prepare datasets\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/utils.py:Load tokenized dataset from cache at ./jo_data_cache_GPT2Tokenizer\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build inputs and labels\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:using premise extra: False\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build instance version: None\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Skipped 26278 cases out of 90426 in the train dataset!!\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:In 0 cases, premise wasn't found in the post\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Skipped 2404 cases out of 8750 in the valid dataset!!\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:In 0 cases, premise wasn't found in the post\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Pad inputs and convert to Tensor\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Build train and validation dataloaders\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Train dataset (Batch, Candidates, Seq length): torch.Size([64148, 2, 509])\n",
      "INFO:/workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py:Valid dataset (Batch, Candidates, Seq length): torch.Size([6346, 2, 509])\n",
      "/usr/local/lib/python3.6/dist-packages/ignite/handlers/checkpoint.py:708: UserWarning: Argument save_interval is deprecated and should be None. This argument will be removed in 0.5.0.Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n",
      "INFO:ignite.engine.engine.Engine:Engine run starting with max_epochs=4.\n",
      "Epoch [1/4]: [7/64148]   0%|                         , loss=12.6 [00:01<4:40:44]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Epoch [1/4]: [29207/64148]  46%|########1         , loss=0.433 [2:13:12<2:37:02]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 $sys.executable /workspace/counter-argument-generation-via-undermining/code/lib/counter_gen_lib/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json \\\n",
    "                    --model_checkpoint gpt2 \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --baseline \\\n",
    "                    --dataset_cache ./jo_data_cache \\\n",
    "                    --output_model_checkpoint jo_baseline_gpt2 \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argument-undermining-env",
   "language": "python",
   "name": "argument-undermining-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
