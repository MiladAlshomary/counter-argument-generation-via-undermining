{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "sys.path.append('/workspace/computationally-undermining-arguments/scripts/')\n",
    "sys.path.append('/workspace/computationally-undermining-arguments/src-py/')\n",
    "sys.path.append('/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import interact\n",
    "import train\n",
    "import utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_, tokenizer = interact.load_model('/workspace/ceph_data/argument-undermining/models/gen_models/Nov02_10-06-11_gammaweb02_full_post_full_counter/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df =  pd.read_json(\"/workspace/ceph_data/argument-undermining/data/gen_training.json\")\n",
    "valid_df =  pd.read_json(\"/workspace/ceph_data/argument-undermining/data/gen_validation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for long counter-premises\n",
    "# training_df =  pd.read_json(\"/workspace/ceph_data/argument-undermining/data/new_gen_training.json\")\n",
    "# valid_df =  pd.read_json(\"/workspace/ceph_data/argument-undermining/data/new_gen_validation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = clean_df(training_df)\n",
    "valid_df = clean_df(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1 = prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=False, context='title', wrong_counter='full_post')\n",
    "json.dump(training_data1, open('/workspace/ceph_data/argument-undermining/data/gpt_data/title_premise.json', 'w'))\n",
    "\n",
    "training_data2 = prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=False, context='full_post', wrong_counter='full_post')\n",
    "json.dump(training_data2, open('/workspace/ceph_data/argument-undermining/data/gpt_data/full_post_premise.json', 'w'))\n",
    "\n",
    "training_data3 = prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=True, context='title', wrong_counter='full_post')\n",
    "json.dump(training_data3, open('/workspace/ceph_data/argument-undermining/data/gpt_data/title_full_counter.json', 'w'))\n",
    "\n",
    "training_data4 = prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=True, context='full_post', wrong_counter='full_post')\n",
    "json.dump(training_data4, open('/workspace/ceph_data/argument-undermining/data/gpt_data/full_post_full_counter.json', 'w'))\n",
    "\n",
    "training_data5 = prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=False, context='title+full_post', wrong_counter='full_post')\n",
    "json.dump(training_data5, open('/workspace/ceph_data/argument-undermining/data/gpt_data/argument_premise.json', 'w'))\n",
    "\n",
    "training_data6 = prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=True, context='title+full_post', wrong_counter='full_post')\n",
    "json.dump(training_data6, open('/workspace/ceph_data/argument-undermining/data/gpt_data/argument_full_counter.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Jo Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "jo_df =  pd.read_pickle(\"/workspace/ceph_data/argument-undermining/jo_data/gen_data.pickle\")\n",
    "\n",
    "training_df = jo_df[jo_df.split =='train']\n",
    "valid_df = jo_df[jo_df.split =='val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/computationally-undermining-arguments/src-py/utility.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[title_clm] = df[title_clm].apply(lambda x : x.strip().lower())\n",
      "/workspace/computationally-undermining-arguments/src-py/utility.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[post_clm] = df[post_clm].apply(lambda x : [sent.strip().lower() for sent in x])\n"
     ]
    }
   ],
   "source": [
    "training_df = utility.clean_df(training_df)\n",
    "valid_df    = utility.clean_df(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "jo_training = utility.prepare_data_for_training({'train': training_df, 'valid': valid_df[0:5000]},\n",
    "                                                full_counter=False, context='title+post', \n",
    "                                                post_clm='post', \n",
    "                                                comment_clm='comment_sents', \n",
    "                                                attacks_clm='premise_counter_premise_pairs', max_sens=20, baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(jo_training, open('/workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data1 = utility.prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=False, context='title')\n",
    "# json.dump(training_data1, open('/workspace/ceph_data/argument-undermining/data/gpt_data/jo_title_premise.json', 'w'))\n",
    "\n",
    "# training_data2 = utility.prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=False, context='full_post')\n",
    "# json.dump(training_data2, open('/workspace/ceph_data/argument-undermining/data/gpt_data/jo_full_post_premise.json', 'w'))\n",
    "\n",
    "# training_data3 = utility.prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=True, context='title')\n",
    "# json.dump(training_data3, open('/workspace/ceph_data/argument-undermining/data/gpt_data/jo_title_full_counter.json', 'w'))\n",
    "\n",
    "# training_data4 = utility.prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=True, context='full_post')\n",
    "# json.dump(training_data4, open('/workspace/ceph_data/argument-undermining/data/gpt_data/jo_full_post_full_counter.json', 'w'))\n",
    "\n",
    "# training_data5 = utility.prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=False, context='title+full_post')\n",
    "# json.dump(training_data5, open('/workspace/ceph_data/argument-undermining/data/gpt_data/jo_argument_premise.json', 'w'))\n",
    "\n",
    "# training_data6 = utility.prepare_data_for_training({'train': training_df, 'valid': valid_df},  full_counter=True, context='title+full_post')\n",
    "# json.dump(training_data6, open('/workspace/ceph_data/argument-undermining/data/gpt_data/jo_argument_full_counter.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GPT2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py\", line 85\n",
      "    if premise_as_str.find(s_str) > -1 or s_str.find(premise_as_str) > -1:\n",
      "                                                                         ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=3 python /workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/argument_full_counter.json \\\n",
    "                    --model_checkpoint openai-gpt \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --output_model_checkpoint argument_full_counter \\\n",
    "                    --dataset_cache ./argument_full_counter_cache \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=4 python /workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/argument_premise.json \\\n",
    "                    --model_checkpoint openai-gpt \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --dataset_cache ./argument_premise_counter_cache \\\n",
    "                    --output_model_checkpoint argument_premise_counter \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1 python /workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/full_post_premise.json \\\n",
    "                    --model_checkpoint openai-gpt \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --dataset_cache ./full_post_premise_counter_cache \\\n",
    "                    --output_model_checkpoint full_post_premise_counter \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1 python /workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/title_premise.json \\\n",
    "                    --model_checkpoint openai-gpt \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --dataset_cache ./title_premise_counter_cache \\\n",
    "                    --output_model_checkpoint title_premise_counter \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=4 python /workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/full_post_full_counter.json \\\n",
    "                    --model_checkpoint openai-gpt \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --dataset_cache ./full_post_full_counter_cache \\\n",
    "                    --output_model_checkpoint full_post_full_counter \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model where weak premise tokens are annotated inside the post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 21:49:37.025234: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "WARNING:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Running process -1\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Arguments: Namespace(baseline=False, build_instance_version='v2', dataset_cache='./jo_data_cache', dataset_path='/workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json', device='cuda', eval_before_start=False, fp16='', gradient_accumulation_steps=8, lm_coef=1.0, local_rank=-1, log_dir='/workspace/ceph_data/argument-undermining/models/gen_models/', lr=6.25e-05, max_history=2, max_norm=1.0, mc_coef=1.0, model_checkpoint='openai-gpt', n_epochs=6, num_candidates=2, output_model_checkpoint='my_approach_v2', personality_permutations=1, premise_extra=False, train_batch_size=1, valid_batch_size=1)\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Prepare tokenizer, pretrained model and optimizer.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "Some weights of OpenAIGPTDoubleHeadsModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Prepare datasets\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/utils.py:Load tokenized dataset from cache at ./jo_data_cache_OpenAIGPTTokenizer\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Build inputs and labels\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:using premise extra: False\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Build instance version: v2\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Skipped 28372 cases out of 90426 in the train dataset!!\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:In 9274 cases, premise wasn't found in the post\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Skipped 2613 cases out of 8750 in the valid dataset!!\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:In 10382 cases, premise wasn't found in the post\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Pad inputs and convert to Tensor\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Build train and validation dataloaders\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Train dataset (Batch, Candidates, Seq length): torch.Size([62054, 2, 509])\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Valid dataset (Batch, Candidates, Seq length): torch.Size([6137, 2, 509])\n",
      "/opt/miniconda3/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:708: UserWarning: Argument save_interval is deprecated and should be None. This argument will be removed in 0.5.0.Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n",
      "INFO:ignite.engine.engine.Engine:Engine run starting with max_epochs=6.\n",
      "/opt/miniconda3/lib/python3.8/site-packages/transformers/modeling_openai.py:688: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
      "  warnings.warn(\n",
      "Epoch [1/6]: [479/62054]   1%|▏                     , loss=0.578 [02:02<4:22:06]"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=5 python /workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json \\\n",
    "                    --model_checkpoint openai-gpt \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --output_model_checkpoint my_approach_v2 \\\n",
    "                    --dataset_cache ./jo_data_cache \\\n",
    "                    --build_instance_version 'v2' \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-21 17:09:26.828200: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "WARNING:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Running process -1\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Arguments: Namespace(baseline=False, build_instance_version='v4', dataset_cache='./jo_data_cache', dataset_path='/workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json', device='cuda', eval_before_start=False, fp16='', gradient_accumulation_steps=8, lm_coef=1.0, local_rank=-1, log_dir='/workspace/ceph_data/argument-undermining/models/gen_models/', lr=6.25e-05, max_history=2, max_norm=1.0, mc_coef=1.0, model_checkpoint='openai-gpt', n_epochs=6, num_candidates=2, output_model_checkpoint='my_approach_v4', personality_permutations=1, premise_extra=False, train_batch_size=1, valid_batch_size=1)\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Prepare tokenizer, pretrained model and optimizer.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "Some weights of OpenAIGPTDoubleHeadsModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight', 'multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Prepare datasets\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/utils.py:Load tokenized dataset from cache at ./jo_data_cache_OpenAIGPTTokenizer\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Build inputs and labels\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:using premise extra: False\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Build instance version: v4\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Skipped 28372 cases out of 90426 in the train dataset!!\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:In 9274 cases, premise wasn't found in the post\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Skipped 2613 cases out of 8750 in the valid dataset!!\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:In 10382 cases, premise wasn't found in the post\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Pad inputs and convert to Tensor\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Build train and validation dataloaders\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Train dataset (Batch, Candidates, Seq length): torch.Size([62054, 2, 511])\n",
      "INFO:/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py:Valid dataset (Batch, Candidates, Seq length): torch.Size([6137, 2, 511])\n",
      "/opt/miniconda3/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:708: UserWarning: Argument save_interval is deprecated and should be None. This argument will be removed in 0.5.0.Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n",
      "INFO:ignite.engine.engine.Engine:Engine run starting with max_epochs=6.\n",
      "/opt/miniconda3/lib/python3.8/site-packages/transformers/modeling_openai.py:688: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
      "  warnings.warn(\n",
      "Epoch [1/6]: [10282/62054]  17%|███▎                , loss=0.471 [44:55<3:46:57]"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=3 python /workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/train.py \\\n",
    "                    --dataset_path /workspace/ceph_data/argument-undermining/data/gpt_data/jo_data.json \\\n",
    "                    --model_checkpoint openai-gpt \\\n",
    "                    --log_dir /workspace/ceph_data/argument-undermining/models/gen_models/ \\\n",
    "                    --output_model_checkpoint my_approach_v4 \\\n",
    "                    --dataset_cache ./jo_data_cache \\\n",
    "                    --build_instance_version 'v4' \\\n",
    "                    --num_candidates 2 \\\n",
    "                    --train_batch_size 1 \\\n",
    "                    --valid_batch_size 1 \\\n",
    "                    --lr 6.25e-5 \\\n",
    "                    --n_epochs 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
