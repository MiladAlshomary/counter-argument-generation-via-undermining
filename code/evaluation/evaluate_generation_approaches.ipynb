{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " %load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bert_score import score\n",
    "from moverscore_v2 import get_idf_dict, word_mover_score \n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "sys.path.append('/workspace/computationally-undermining-arguments/scripts/')\n",
    "sys.path.append('/workspace/computationally-undermining-arguments/src-py')\n",
    "sys.path.append('/workspace/computationally-undermining-arguments/thirdparty/transfer-learning-conv-ai/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import evaluation\n",
    "import json\n",
    "import utility\n",
    "from argparse import Namespace\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df  = pd.read_pickle('./testing_df.pkl')\n",
    "expanded_df = pd.read_pickle('./expanded_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108600</th>\n",
       "      <td>[[[if you agree with this, my point is, this does not connect at all to our perception of good., if it did, god would not allow insert horrible event that happens daily in real life and encourage insert horrible bible verse about slavery women war punishment .], you have a really weird view of good and bad. if you think that the world is inherently bad, and your viewpoint is wrong, maybe you shouldn't have your view changed to reflect this view. but then there is a good reason to believe that heaven and hell are both bad. and that there is no good in them. you can't blame'bad'god for their existence.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105230</th>\n",
       "      <td>[[[i. e. , the central bank gives money to your local bank so your local bank can lend it to you at low cost.], can you elaborate on why you think this? if your first argument is that inflation is increasing, i will definitely dispute that. the main problem is that people are constantly looking at the stock market, trying to figure out how the stock market can function. in the process, people can buy or buy bonds for a reason. it is very possible that the market is getting smaller, and that the market is getting larger.], [[the problem with this logic is it results in wage growth being the last benefactor of all this cheap money if at all.], where are the profits for this? why can't the government pay wages higher if people are working to increase the wage of their income? if there is a demand for'low interest rates'and they don't get any interest, then the average person would be low in wage and unable to move money around to pay prices. you aren't taking any of these into account. as it stands, the stock market is not rising, and the market is still growing.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109572</th>\n",
       "      <td>[[[4 accepting lgbt might mean accepting other degenerates if we accept these people today who guarantees we arent accepting zoophiles and pedos tomorrow?], because they didn't want to be trans, which is the exact opposite of the lgbt issue, which you were referring to earlier. it's also not about allowing homosexuals to live in society. they want to be around and provide for them, a place for their children and, most importantly, for the general population. in order for such things to happen, they need to be able to make a living from it, including working to pay for what they do. you seem to think this's a deal breaker, but it's not.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110783</th>\n",
       "      <td>[[[i think beastiality should be illegal only because it disgusts me, as ridiculous as that sounds.], why? if you find something disgusting and you find it disgusting, but you're still ok with doing so, why should people find it morally wrong. is that not your view? is it your moral position to kill animals because you think it disgusts you? if you think it disgusts me, then you can't justify any of it.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103458</th>\n",
       "      <td>[[[there are 500m who do nothing but send transactions and eat salmon and all the other people are cooking the salmon and making sure the transaction goes well.], how are you quantifying 500m of wealth? most people don't know what 500m is, or the number of hours it takes to feed, feed, and feed a single person. the total amount of money you spend at work has nothing to do with the amount of money you save. it is entirely possible for a person to make 50, 000 a year and not do anything else besides just do it on the off chance you are in that position for the rest of their life. there are still a number of people who are good at something even if they are terrible at it.]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        app_v2\n",
       "108600                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [[[if you agree with this, my point is, this does not connect at all to our perception of good., if it did, god would not allow insert horrible event that happens daily in real life and encourage insert horrible bible verse about slavery women war punishment .], you have a really weird view of good and bad. if you think that the world is inherently bad, and your viewpoint is wrong, maybe you shouldn't have your view changed to reflect this view. but then there is a good reason to believe that heaven and hell are both bad. and that there is no good in them. you can't blame'bad'god for their existence.]]\n",
       "105230  [[[i. e. , the central bank gives money to your local bank so your local bank can lend it to you at low cost.], can you elaborate on why you think this? if your first argument is that inflation is increasing, i will definitely dispute that. the main problem is that people are constantly looking at the stock market, trying to figure out how the stock market can function. in the process, people can buy or buy bonds for a reason. it is very possible that the market is getting smaller, and that the market is getting larger.], [[the problem with this logic is it results in wage growth being the last benefactor of all this cheap money if at all.], where are the profits for this? why can't the government pay wages higher if people are working to increase the wage of their income? if there is a demand for'low interest rates'and they don't get any interest, then the average person would be low in wage and unable to move money around to pay prices. you aren't taking any of these into account. as it stands, the stock market is not rising, and the market is still growing.]]\n",
       "109572                                                                                                                                                                                                                                                                                                                                                                                                                                                   [[[4 accepting lgbt might mean accepting other degenerates if we accept these people today who guarantees we arent accepting zoophiles and pedos tomorrow?], because they didn't want to be trans, which is the exact opposite of the lgbt issue, which you were referring to earlier. it's also not about allowing homosexuals to live in society. they want to be around and provide for them, a place for their children and, most importantly, for the general population. in order for such things to happen, they need to be able to make a living from it, including working to pay for what they do. you seem to think this's a deal breaker, but it's not.]]\n",
       "110783                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [[[i think beastiality should be illegal only because it disgusts me, as ridiculous as that sounds.], why? if you find something disgusting and you find it disgusting, but you're still ok with doing so, why should people find it morally wrong. is that not your view? is it your moral position to kill animals because you think it disgusts you? if you think it disgusts me, then you can't justify any of it.]]\n",
       "103458                                                                                                                                                                                                                                                                                                                                                                                                                [[[there are 500m who do nothing but send transactions and eat salmon and all the other people are cooking the salmon and making sure the transaction goes well.], how are you quantifying 500m of wealth? most people don't know what 500m is, or the number of hours it takes to feed, feed, and feed a single person. the total amount of money you spend at work has nothing to do with the amount of money you save. it is entirely possible for a person to make 50, 000 a year and not do anything else besides just do it on the off chance you are in that position for the rest of their life. there are still a number of people who are good at something even if they are terrible at it.]]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df[['app_v2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_on_df(df_gt, df_preds):\n",
    "    bleu_1, bleu1_scores = evaluation.eval_bleu(df_gt, df_preds, weights=(1,0,0,0))\n",
    "    bleu, bleu_scores  = evaluation.eval_bleu(df_gt, df_preds, weights=(0.5, 0.5,0,0))\n",
    "    meteor, meteor_scores = evaluation.eval_meteor(df_gt, df_preds)\n",
    "    return [meteor, bleu_1, bleu], [bleu1_scores, bleu_scores, meteor_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_multiple_preds(df, pred_clms, gt_clm):\n",
    "    gt_texts = df[gt_clm].tolist()\n",
    "    \n",
    "    bleu1_evals  = []\n",
    "    bleu_evals   = []\n",
    "    meteor_evals = []\n",
    "    \n",
    "    for clm in pred_clms:\n",
    "        bleu1_evals.append(evaluation.eval_bleu(gt_texts, df[clm].tolist(), weights=(1,0,0,0)))\n",
    "        bleu_evals.append(evaluation.eval_bleu(gt_texts, df[clm].tolist(), weights=(0.5, 0.5,0,0)))\n",
    "        meteor_evals.append(evaluation.eval_meteor(gt_texts, df[clm].tolist()))\n",
    "    \n",
    "    bleu1_score = round(np.mean([x[0] for x in bleu1_evals]), 3)\n",
    "    bleu_score = round(np.mean([x[0] for x in bleu_evals]), 3)\n",
    "    meteor_score = round(np.mean([x[0] for x in meteor_evals]), 3)\n",
    "    \n",
    "    bleu1_scores  = np.mean([x[1] for x in bleu1_evals], axis=0)\n",
    "    bleu_scores   = np.mean([x[1] for x in bleu_evals], axis=0)\n",
    "    meteor_scores = np.mean([x[1] for x in meteor_evals], axis=0)\n",
    "    \n",
    "    return [meteor_score, bleu1_score, bleu_score], [bleu1_scores, bleu_scores, meteor_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_match(df, pred_clm, gt_clm):\n",
    "    pred_attacks = df[pred_clm].tolist()\n",
    "    gt_attacks   = df[gt_clm].tolist()\n",
    "    \n",
    "    best_preds = []\n",
    "    for post_pred_attacks, post_gt_attacks in zip(pred_attacks, gt_attacks):\n",
    "        attack_overlaps = [utility.overlap_between_attack_and_attacked_premises(weak_premise, attack) for weak_premise, attack in post_pred_attacks]\n",
    "        best_attack_idx = attack_overlaps.index(max(attack_overlaps))\n",
    "        best_attack = post_pred_attacks[best_attack_idx]\n",
    "        gt_attack   = post_gt_attacks[best_attack_idx]\n",
    "        best_preds.append([best_attack[0], best_attack[1], gt_attack[1]])\n",
    "\n",
    "    return best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df['app_v2_best_match'] = choose_best_match(testing_df, 'app_v2', 'premise_counter_premise_pairs')\n",
    "testing_df['app_v3_best_match'] = choose_best_match(testing_df, 'app_v3', 'premise_counter_premise_pairs')\n",
    "testing_df['baseline_best_match'] = choose_best_match(testing_df, 'baseline_final', 'premise_counter_premise_pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, app_v2_gt, app_v2_pred = zip(*testing_df.app_v2_best_match.tolist())\n",
    "tmp_app_v2_score, tmp_app_v2_scores = eval_model_on_df(app_v2_gt, app_v2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, app_v3_gt, app_v3_pred = zip(*testing_df.app_v3_best_match.tolist())\n",
    "tmp_app_v3_score, tmp_app_v3_scores = eval_model_on_df(app_v3_gt, app_v3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, baseline_gt, baseline_pred = zip(*testing_df.baseline_best_match.tolist())\n",
    "tmp_baseline_score, tmp_baseline_scores = eval_model_on_df(baseline_gt, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach       meteor    bleu-1    bleu-1&2\n",
      "-----------  --------  --------  ----------\n",
      "1. Baseline     0.067    12.646       3.042\n",
      "2. Ours v2      0.066    13.139       3.395\n",
      "3. Ours v3      0.067    12.888       3.274\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([\n",
    "        ['1. Baseline'] + tmp_baseline_score,\n",
    "        ['2. Ours v2'] + tmp_app_v2_score,\n",
    "        ['3. Ours v3'] + tmp_app_v3_score,\n",
    "       ], headers=['Approach', 'meteor', 'bleu-1', 'bleu-1&2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model's predictions against the ground-truth premises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalute model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_evals, baseline_scores= eval_model_on_df(expanded_df.gt_counter.tolist(), expanded_df.baseline_final.tolist())\n",
    "app_v2_evals, app_v2_scores = eval_model_on_df(expanded_df.gt_counter.tolist(), expanded_df.app_v2.tolist())\n",
    "app_v3_evals, app_v3_scores = eval_model_on_df(expanded_df.gt_counter.tolist(), expanded_df.app_v3.tolist())\n",
    "app_v4_evals, app_v4_scores = eval_model_on_df(expanded_df.gt_counter.tolist(), expanded_df.app_v4.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v2_title_evals, app_v2_title_scores = eval_model_on_df(expanded_df.gt_counter.tolist(), expanded_df.app_v2_title.tolist())\n",
    "app_v3_title_evals, app_v3_title_scores = eval_model_on_df(expanded_df.gt_counter.tolist(), expanded_df.app_v3_title.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v4_title_evals, app_v4_title_scores = eval_model_on_df(expanded_df.gt_counter.tolist(), expanded_df.app_v4_title.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v2_random_baseline_evals, app_v2_random_baseline_scores = eval_multiple_preds(expanded_df, ['app_v2_rand_0', 'app_v2_rand_1', 'app_v2_rand_2'], 'gt_counter')\n",
    "app_v3_random_baseline_evals, app_v3_random_baseline_scores = eval_multiple_preds(expanded_df, ['app_v3_rand_0', 'app_v3_rand_1', 'app_v3_rand_2'], 'gt_counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v4_random_baseline_evals, app_v4_random_baseline_scores = eval_multiple_preds(expanded_df, ['app_v4_rand_0', 'app_v4_rand_1', 'app_v4_rand_2'], 'gt_counter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model's predictions against the ground-truth comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_evals_against_args, baseline_scores_against_args= eval_model_on_df(expanded_df.all_counter_arguments.tolist(), expanded_df.baseline_final)\n",
    "app_v2_evals_against_args, app_v2_scores_against_args = eval_model_on_df(expanded_df.all_counter_arguments.tolist(), expanded_df.app_v2.tolist())\n",
    "app_v3_evals_against_args, app_v3_scores_against_args = eval_model_on_df(expanded_df.all_counter_arguments.tolist(), expanded_df.app_v3.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v4_evals_against_args, app_v4_scores_against_args = eval_model_on_df(expanded_df.all_counter_arguments.tolist(), expanded_df.app_v4.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v2_random_baseline_evals_against_args, app_v2_random_baseline_scores_against_args = eval_multiple_preds(expanded_df, ['app_v2_rand_0', 'app_v2_rand_1', 'app_v2_rand_2'], 'all_counter_arguments')\n",
    "app_v3_random_baseline_evals_against_args, app_v3_random_baseline_scores_against_args = eval_multiple_preds(expanded_df, ['app_v3_rand_0', 'app_v3_rand_1', 'app_v3_rand_2'], 'all_counter_arguments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v4_random_baseline_evals_against_args, app_v4_random_baseline_scores_against_args = eval_multiple_preds(expanded_df, ['app_v4_rand_0', 'app_v4_rand_1', 'app_v4_rand_2'], 'all_counter_arguments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v2_title_evals_against_args, app_v2_title_scores_against_args = eval_model_on_df(expanded_df.all_counter_arguments.tolist(), expanded_df.app_v2_title.tolist())\n",
    "app_v3_title_evals_against_args, app_v3_title_scores_against_args = eval_model_on_df(expanded_df.all_counter_arguments.tolist(), expanded_df.app_v3_title.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_v4_title_evals_against_args, app_v4_title_scores_against_args = eval_model_on_df(expanded_df.all_counter_arguments.tolist(), expanded_df.app_v4_title.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach               meteor    bleu-1    bleu-1&2    meteor    bleu-1    bleu-1&2\n",
      "-------------------  --------  --------  ----------  --------  --------  ----------\n",
      "1. Baseline             0.058    13.023       3.117     0.097    10.4         3.212\n",
      "2. Ours v2 - title      0.06     12.532       2.943     0.09      9.472       2.837\n",
      "2. Ours v2 - random     0.058    12.838       3.005     0.096    10.398       3.255\n",
      "2. Ours v2              0.057    13.453       3.391     0.102    10.998       3.764\n",
      "3. Ours v3 - title      0.06     12.635       3.023     0.092     9.685       2.984\n",
      "3. Ours v3 - random     0.059    12.712       2.987     0.096    10.161       3.217\n",
      "3. Ours v3              0.058    13.162       3.217     0.101    10.743       3.651\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([\n",
    "                ['1. Baseline'] + baseline_evals + baseline_evals_against_args,\n",
    "                ['2. Ours v2 - title'] + app_v2_title_evals + app_v2_title_evals_against_args,\n",
    "                ['2. Ours v2 - random']+ app_v2_random_baseline_evals + app_v2_random_baseline_evals_against_args,\n",
    "                ['2. Ours v2'] + app_v2_evals + app_v2_evals_against_args,\n",
    "                ['3. Ours v3 - title'] + app_v3_title_evals + app_v3_title_evals_against_args,\n",
    "                ['3. Ours v3 - random'] + app_v3_random_baseline_evals + app_v3_random_baseline_evals_against_args,\n",
    "                ['3. Ours v3'] + app_v3_evals + app_v3_evals_against_args,\n",
    "                #['4. Ours v4 - title'] + app_v4_title_evals + app_v4_title_evals_against_args,\n",
    "                #['4. Ours v4 - random'] + app_v4_random_baseline_evals + app_v4_random_baseline_evals_against_args,\n",
    "                #['4. Ours v4'] + app_v4_evals + app_v4_evals_against_args,\n",
    "               ], headers=['Approach', 'meteor', 'bleu-1', 'bleu-1&2', 'meteor', 'bleu-1', 'bleu-1&2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app2 vs baseline:\n",
      "{'bleu-1': {'@5%': True, '%10': True}, 'bleu': {'@5%': True, '%10': True}, 'meteor': {'@5%': False, '%10': False}}\n",
      "{'bleu-1': {'@5%': True, '%10': True}, 'bleu': {'@5%': True, '%10': True}, 'meteor': {'@5%': True, '%10': True}}\n"
     ]
    }
   ],
   "source": [
    "# print(perform_significance_tests(baseline_scores, model_1_scores))\n",
    "# print(perform_significance_tests(baseline_scores, model_2_scores))\n",
    "# print(perform_significance_tests(baseline_scores, model_3_scores))\n",
    "# print(perform_significance_tests(baseline_scores, model_4_scores))\n",
    "# print(perform_significance_tests(baseline_scores, model_5_scores))\n",
    "print('app2 vs baseline:')\n",
    "print(perform_significance_tests(baseline_scores, app_v2_scores))\n",
    "print(perform_significance_tests(baseline_scores_against_args, app_v2_scores_against_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app3 vs baseline:\n",
      "{'bleu-1': {'@5%': False, '%10': False}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': False, '%10': False}}\n",
      "{'bleu-1': {'@5%': True, '%10': True}, 'bleu': {'@5%': True, '%10': True}, 'meteor': {'@5%': True, '%10': True}}\n"
     ]
    }
   ],
   "source": [
    "print('app3 vs baseline:')\n",
    "print(perform_significance_tests(baseline_scores, app_v3_scores))\n",
    "print(perform_significance_tests(baseline_scores_against_args, app_v3_scores_against_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app4 vs baseline:\n",
      "{'bleu-1': {'@5%': False, '%10': False}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': False, '%10': False}}\n",
      "{'bleu-1': {'@5%': True, '%10': True}, 'bleu': {'@5%': True, '%10': True}, 'meteor': {'@5%': False, '%10': True}}\n"
     ]
    }
   ],
   "source": [
    "print('app4 vs baseline:')\n",
    "print(perform_significance_tests(baseline_scores, app_v4_scores))\n",
    "print(perform_significance_tests(baseline_scores_against_args, app_v4_scores_against_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app2 vs app3:\n",
      "{'bleu-1': {'@5%': True, '%10': True}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': True, '%10': True}}\n",
      "{'bleu-1': {'@5%': False, '%10': True}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': True, '%10': True}}\n"
     ]
    }
   ],
   "source": [
    "print('app2 vs app3:')\n",
    "print(perform_significance_tests(app_v3_scores, app_v2_scores))\n",
    "print(perform_significance_tests(app_v3_scores_against_args, app_v2_scores_against_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app2 vs appv2 random:\n",
      "{'bleu-1': {'@5%': False, '%10': False}, 'bleu': {'@5%': True, '%10': True}, 'meteor': {'@5%': False, '%10': True}}\n",
      "{'bleu-1': {'@5%': False, '%10': False}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': False, '%10': False}}\n"
     ]
    }
   ],
   "source": [
    "print('app2 vs appv2 random:')\n",
    "print(perform_significance_tests(app_v2_scores, [x[0] for x in app_v2_random_baseline_scores]))\n",
    "print(perform_significance_tests(app_v2_scores_against_args, [x[0] for x in  app_v2_random_baseline_scores_against_args]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app3 vs appv3 random:\n",
      "{'bleu-1': {'@5%': False, '%10': True}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': False, '%10': False}}\n",
      "{'bleu-1': {'@5%': True, '%10': True}, 'bleu': {'@5%': True, '%10': True}, 'meteor': {'@5%': True, '%10': True}}\n"
     ]
    }
   ],
   "source": [
    "print('app3 vs appv3 random:')\n",
    "print(perform_significance_tests(app_v3_scores, [x[0] for x in app_v3_random_baseline_scores]))\n",
    "print(perform_significance_tests(app_v3_scores_against_args, [x[0] for x in app_v3_random_baseline_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app2 vs appv2 title:\n",
      "{'bleu-1': {'@5%': False, '%10': False}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': False, '%10': False}}\n",
      "{'bleu-1': {'@5%': False, '%10': False}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': False, '%10': False}}\n"
     ]
    }
   ],
   "source": [
    "print('app2 vs appv2 title:')\n",
    "print(perform_significance_tests(app_v2_scores, app_v2_title_scores))\n",
    "print(perform_significance_tests(app_v2_scores_against_args, app_v2_title_scores_against_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app3 vs appv3 title:\n",
      "{'bleu-1': {'@5%': False, '%10': False}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': True, '%10': True}}\n",
      "{'bleu-1': {'@5%': False, '%10': False}, 'bleu': {'@5%': False, '%10': False}, 'meteor': {'@5%': False, '%10': False}}\n"
     ]
    }
   ],
   "source": [
    "print('app3 vs appv3 title:')\n",
    "print(perform_significance_tests(app_v3_scores, app_v3_title_scores))\n",
    "print(perform_significance_tests(app_v3_scores_against_args, app_v3_title_scores_against_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_between_attack_and_attacked_premises(attacked_premises, attack):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    en_stopwords = stopwords.words('english')\n",
    "\n",
    "    attack_tokens =[token for token in nltk.word_tokenize(attack) if token not in en_stopwords]\n",
    "    \n",
    "    attacked_premise_tokens = []\n",
    "    for sentence in attacked_premises:\n",
    "        attacked_premise_tokens += [token for token in nltk.word_tokenize(sentence) if token not in en_stopwords]\n",
    "\n",
    "    return len(set(attack_tokens).intersection(attacked_premise_tokens))/len(set(attacked_premise_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df['app2_overlap'] = expanded_df.apply(lambda row: overlap_between_attack_and_attacked_premises(row['premise'], row['app_v2']), axis=1)\n",
    "expanded_df['app3_overlap'] = expanded_df.apply(lambda row: overlap_between_attack_and_attacked_premises(row['premise'], row['app_v3']), axis=1)\n",
    "expanded_df['baseline_overlap'] = expanded_df.apply(lambda row: overlap_between_attack_and_attacked_premises(row['premise'], row['baseline_final']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline:  0.22\n",
      "appv2:  0.27\n",
      "appv3:  0.27\n"
     ]
    }
   ],
   "source": [
    "print('baseline: ', round(np.mean(expanded_df['baseline_overlap'].tolist()), 2))\n",
    "print('appv2: ', round(np.mean(expanded_df['app2_overlap'].tolist()), 2))\n",
    "print('appv3: ', round(np.mean(expanded_df['app3_overlap'].tolist()), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "expanded_df.baseline_overlap.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.baseline_overlap.tolist(), \n",
    "          expanded_df.app2_overlap.tolist(), expanded_df.app3_overlap.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([207, 650, 476, 161,  62,  55,  11,   6,   4]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]))"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(expanded_df.baseline_overlap.tolist(), bins=np.arange(0, 1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([144, 449, 499, 233, 131, 125,  29,   9,  10]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]))"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(expanded_df.app2_overlap.tolist(), bins=np.arange(0, 1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([137, 467, 486, 240, 126, 115,  26,  19,  13]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]))"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(expanded_df.app3_overlap.tolist(), bins=np.arange(0, 1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.hist([expanded_df.baseline_overlap.tolist(), \n",
    "          expanded_df.app2_overlap.tolist(), expanded_df.app3_overlap.tolist()], \n",
    "         alpha=1.0, label=['counter-baseline', 'our-model-w/o', 'our-model-w'], bins=np.arange(0,1, 0.2))\n",
    "plt.xticks(np.arange(0,1, 0.2))\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('line_plot.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "expanded_df['app2_overlap'] = expanded_df.apply(lambda row: utility.overlap_between_attack_and_attacked_premises(row['post'], row['app_v2']), axis=1)\n",
    "expanded_df['app3_overlap'] = expanded_df.apply(lambda row: utility.overlap_between_attack_and_attacked_premises(row['post'], row['app_v3']), axis=1)\n",
    "expanded_df['baseline_overlap'] = expanded_df.apply(lambda row: utility.overlap_between_attack_and_attacked_premises(row['post'], row['baseline_final']), axis=1)\n",
    "\n",
    "plt.hist([expanded_df.baseline_overlap.tolist(), \n",
    "          expanded_df.app2_overlap.tolist(), expanded_df.app3_overlap.tolist()], \n",
    "         alpha=1.0, label=['baseline', 'ours v2', 'ours v3'], bins=np.arange(0,1, 0.2))\n",
    "plt.xticks(np.arange(0,1, 0.2))\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "expanded_df['app2_overlap'] = expanded_df.apply(lambda row: utility.overlap_between_attack_and_attacked_premises([row['gt_counter']], row['app_v2']), axis=1)\n",
    "expanded_df['app3_overlap'] = expanded_df.apply(lambda row: utility.overlap_between_attack_and_attacked_premises([row['gt_counter']], row['app_v3']), axis=1)\n",
    "expanded_df['baseline_overlap'] = expanded_df.apply(lambda row: utility.overlap_between_attack_and_attacked_premises([row['gt_counter']], row['baseline_final']), axis=1)\n",
    "\n",
    "plt.hist([expanded_df.baseline_overlap.tolist(), \n",
    "          expanded_df.app2_overlap.tolist(), expanded_df.app3_overlap.tolist()], \n",
    "         alpha=1.0, label=['baseline', 'ours v2', 'ours v3'], bins=np.arange(0,1, 0.2))\n",
    "plt.xticks(np.arange(0,1, 0.2))\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = expanded_df[['title', 'post', 'premise', 'gt_counter', 'app_v2', 'app_v3', 'app_v4', 'baseline_final']].sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "['title', 'post', 'premise', 'gt_counter', 'ours_with_token_type', 'ours_with_token_type_and_special_tokens', 'ours_with_special_token', 'baseline']\n",
    "['title', 'post', 'premise', 'gt_counter', 'predictions_1', 'predictions_2', 'predictions_3', 'predictions_4']\n",
    "\n",
    "sample_df.columns = ['title', 'post', 'premise', 'gt_counter', 'predictions_1', 'predictions_2', 'predictions_3', 'predictions_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_vis(df, models_names, output_path):\n",
    "    \n",
    "    def annotate_premise(p, weak_premises):\n",
    "        for x in weak_premises:\n",
    "            if x in p or p in x:\n",
    "                return '<weak> ' + p\n",
    "        return p\n",
    "    \n",
    "    #documents\n",
    "    documents = [{'claim': row['title'],\n",
    "                  'weak_premise': row['premise'],\n",
    "                  'premises': row['post'] } for idx, row in df.iterrows()]\n",
    "    \n",
    "    with open(output_path+'documents.json', 'w') as f:\n",
    "        for item in documents:\n",
    "            f.write(json.dumps(item))\n",
    "            f.write('\\n')\n",
    "\n",
    "    #reference\n",
    "    references = df.gt_counter.tolist()\n",
    "    with open(output_path+'references.txt', 'w') as f:\n",
    "        for item in references:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    for model in models_names:\n",
    "        #model_json = {'model': model, 'items': [{'document': row['title'] + '  ------  ' + ' '.join(row['post']), 'reference': row['premise'], 'prediction': row[model]} for idx, row in df.iterrows()]}\n",
    "        #json.dump(model_json, open(output_path+model+'.json', 'w'))\n",
    "        \n",
    "        preds = df[model].tolist()\n",
    "        with open(output_path+model+'.txt', 'w') as f:\n",
    "            for item in preds:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_for_vis(sample_df, ['predictions_1', 'predictions_2', 'predictions_3', 'predictions_4'], './data_files/predictions/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing best approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_folder = './data_files/annotations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "milad_ranks =[x.strip().split('\\t')[1] for x in  open(ann_folder + 'choosing_best_approach - milad-sheet.tsv').readlines()[1:]]\n",
    "milad_ranks =[[int(x) for x in ranks.split(',')] for ranks in  milad_ranks]\n",
    "shahbaz_ranks = [x.strip().split('\\t')[1] for x in  open(ann_folder + 'choosing_best_approach - shahbaz-sheet.tsv').readlines()[1:]]\n",
    "shahbaz_ranks =[[int(x) for x in ranks.split(',')] for ranks in  shahbaz_ranks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ranks = milad_ranks + shahbaz_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common approach in first rank:  1\n",
      "most common approach in second rank:  1\n",
      "most common approach in third rank:  3\n"
     ]
    }
   ],
   "source": [
    "first_ranks = [x[0] for x in all_ranks]\n",
    "print('most common approach in first rank: ',max(set(first_ranks), key=first_ranks.count))\n",
    "      \n",
    "second_ranks = [x[1] for x in all_ranks]\n",
    "print('most common approach in second rank: ',max(set(second_ranks), key=second_ranks.count))\n",
    "      \n",
    "third_ranks = [x[2] for x in all_ranks]\n",
    "print('most common approach in third rank: ',max(set(third_ranks), key=third_ranks.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for app in [1,2,3]:\n",
    "    app_ranks = ['predictions-{}'.format(app)]\n",
    "    for rank in [1,2,3]:\n",
    "        app_perc = [x[rank-1] for x in all_ranks].count(app)/100\n",
    "        app_ranks.append(app_perc)\n",
    "    data.append(app_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 rank-1    rank-2    rank3\n",
      "-------------  --------  --------  -------\n",
      "predictions-1      0.5       0.41     0.09\n",
      "predictions-2      0.41      0.4      0.19\n",
      "predictions-3      0.09      0.19     0.72\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(data, headers=['rank-1', 'rank-2', 'rank3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline vs Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from nltk import agreement\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making it like a ranking list, so if user picked 1 then the ranking will be [1, 4]\n",
    "def extend_s(s):\n",
    "    o = []\n",
    "    for x in s:\n",
    "        o.append(x)\n",
    "        o.append('1' if x=='4' else '4')\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "timon_ranks =[x.strip().split(',')[1:] for x in  open(ann_folder + 'timon_eval_template.csv').readlines()[1:]]\n",
    "max_ranks =[x.strip().split(',')[1:] for x in  open(ann_folder + 'max_eval_template.csv').readlines()[1:]]\n",
    "weifan_ranks =[x.strip().split(',')[1:] for x in  open(ann_folder + 'weifan_eval_template.csv').readlines()[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_scores = [[x[0][0], x[1][0], x[2][0]] for x in zip(timon_ranks, max_ranks, weifan_ranks)]\n",
    "app_scores = [[x[0][1], x[1][1], x[2][1]] for x in zip(timon_ranks, max_ranks, weifan_ranks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant to the weak premise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach was selected: 0.56\n",
      "Baseline was selected: 0.44\n"
     ]
    }
   ],
   "source": [
    "majority_vote = [Counter(x).most_common()[0][0] for x in rel_scores]\n",
    "\n",
    "print('Approach was selected:', majority_vote.count('1')/50)\n",
    "print('Baseline was selected:', majority_vote.count('4')/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach was selected: 0.5733333333333334\n",
      "Baseline was selected: 0.4266666666666667\n"
     ]
    }
   ],
   "source": [
    "absolute_scores = [s for row in rel_scores for s in row]\n",
    "print('Approach was selected:', absolute_scores.count('1')/150)\n",
    "print('Baseline was selected:', absolute_scores.count('4')/150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appropriatness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach was selected: 0.56\n",
      "Baseline was selected: 0.44\n"
     ]
    }
   ],
   "source": [
    "majority_vote = [Counter(x).most_common()[0][0] for x in app_scores]\n",
    "\n",
    "print('Approach was selected:', majority_vote.count('1')/50)\n",
    "print('Baseline was selected:', majority_vote.count('4')/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach was selected: 0.5666666666666667\n",
      "Baseline was selected: 0.43333333333333335\n"
     ]
    }
   ],
   "source": [
    "absolute_scores = [s for row in app_scores for s in row]\n",
    "print('Approach was selected:', absolute_scores.count('1')/150)\n",
    "print('Baseline was selected:', absolute_scores.count('4')/150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = rel_scores + app_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Kendalls for the task of relevancy:  0.4133333333333334\n"
     ]
    }
   ],
   "source": [
    "ann1, ann2, ann3 = zip(*rel_scores)\n",
    "ann1 = extend_s(ann1)\n",
    "ann2 = extend_s(ann2)\n",
    "ann3 = extend_s(ann3)\n",
    "print('Avg Kendalls for the task of relevancy: ', (kendalltau(ann1, ann2)[0] + kendalltau(ann1, ann3)[0] + kendalltau(ann2, ann3)[0])/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Kendalls for the task of appropriateness:  0.22666666666666668\n"
     ]
    }
   ],
   "source": [
    "ann1, ann2, ann3 = zip(*app_scores)\n",
    "ann1 = extend_s(ann1)\n",
    "ann2 = extend_s(ann2)\n",
    "ann3 = extend_s(ann3)\n",
    "print('Avg Kendalls for the task of appropriateness: ', (kendalltau(ann1, ann2)[0] + kendalltau(ann1, ann3)[0] + kendalltau(ann2, ann3)[0])/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
