{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electoral-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "racial-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import *\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_path='/workspace/ceph_data/argument-undermining/data/vul_detection'\n",
    "path_to_jo_predictions= '/workspace/ceph_data/argument-undermining/data/vul_detection/jo_approach_predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extensive-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_significance_tests(app1_preds, app2_preds):\n",
    "    all_data = np.array(list(zip(app1_preds, app2_preds)))\n",
    "    chunks = np.array_split(all_data, 10)\n",
    "    \n",
    "    app1_chunks_scores = []\n",
    "    app2_chunks_scores = []\n",
    "    for chunk in chunks:\n",
    "        app1_chunk, app2_chunk = list(zip(*chunk))\n",
    "        app1_chunks_scores.append((prec_at(app1_chunk, k=1),  prec_at(app1_chunk, k=3)))\n",
    "        app2_chunks_scores.append((prec_at(app2_chunk, k=1),  prec_at(app2_chunk, k=3)))\n",
    "        \n",
    "\n",
    "    sig_report = {}\n",
    "    for idx, measure in enumerate(['P@1', 'Acc@3']):\n",
    "        s1 = [round(s[idx], 3) for s in app1_chunks_scores]\n",
    "        s2 = [round(s[idx], 3) for s in app2_chunks_scores]\n",
    "\n",
    "        sig_report[measure] = {'@5%':check_sig(s2, s1, alpha=0.05), \n",
    "                               '%10': check_sig(s2, s1, alpha=0.1)\n",
    "        }\n",
    "        \n",
    "    return sig_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-sister",
   "metadata": {},
   "source": [
    "Load predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acquired-absolute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "durable-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = json.load(open(predictions_path + '/jo_testing.json'))\n",
    "preds_listwise = json.load(open(predictions_path+'/listwise-pred-jo-test.json'))\n",
    "preds_pointwise= json.load(open(predictions_path+'/pointwise-pred-jo-test.json'))\n",
    "preds_pairwise = json.load(open(predictions_path+'/pairwise-pred-jo-test.json'))\n",
    "jo_preds_df    = pd.read_csv(path_to_jo_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greenhouse-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load jo preds and convert them to the same structure of ours..\n",
    "jo_preds_df['sentence'] = jo_preds_df.apply(lambda row: {'sen_id': row['sentence_no'], 'relevance' : row['y_true'], 'score': row['y_pred']}, axis=1)\n",
    "jo_preds_df = jo_preds_df.groupby('post_id').agg({'sentence': lambda x: list(x)}).reset_index()\n",
    "\n",
    "jo_preds = {'rankingProblemsOutput': []}\n",
    "for idx, post in jo_preds_df.iterrows():\n",
    "    jo_preds['rankingProblemsOutput'].append({'queryText': '', 'documents': post['sentence']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "flexible-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "listwise_model_scores  = [ prec_at(preds_listwise['rankingProblemsOutput'], k=1),  prec_at(preds_listwise['rankingProblemsOutput'], k=3)]\n",
    "pointwise_model_scores = [ prec_at(preds_pointwise['rankingProblemsOutput'], k=1),  prec_at(preds_pointwise['rankingProblemsOutput'], k=3)]\n",
    "pairwise_model_scores = [ prec_at(preds_pairwise['rankingProblemsOutput'], k=1),  prec_at(preds_pairwise['rankingProblemsOutput'], k=3)]\n",
    "jo_scores = [ prec_at(jo_preds['rankingProblemsOutput'], k=1),  prec_at(jo_preds['rankingProblemsOutput'], k=3)]\n",
    "baseline_scores_sen_length = [ prec_at(gt_data['rankingProblems'], baseline='sen_length', k=1),  prec_at(gt_data['rankingProblems'], baseline='sen_length', k=3)]\n",
    "baseline_scores_random = [ prec_at(gt_data['rankingProblems'], baseline='random', k=1),  prec_at(gt_data['rankingProblems'], baseline='random', k=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pleased-repeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#                        P@1    A@3\n",
      "---------------------  -----  -----\n",
      "Sentence Length        0.425  0.738\n",
      "Random                 0.361  0.623\n",
      "Jo et al.              0.487  0.777\n",
      "LTR-bert (point-wise)  0.505  0.787\n",
      "LTR-bert (pair-wise)   0.498  0.78\n",
      "LTR-bert (list-wise)   0.506  0.786\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([\n",
    "    ['Sentence Length'] + baseline_scores_sen_length,\n",
    "    ['Random'] + baseline_scores_random,\n",
    "    ['Jo et al.']+ jo_scores,\n",
    "    ['LTR-bert (point-wise)'] + pointwise_model_scores,\n",
    "    ['LTR-bert (pair-wise)'] + pairwise_model_scores,\n",
    "    ['LTR-bert (list-wise)'] + listwise_model_scores,\n",
    "], headers=['#', 'P@1', 'A@3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "magnetic-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P@1': {'@5%': True, '%10': True}, 'Acc@3': {'@5%': True, '%10': True}}\n",
      "{'P@1': {'@5%': True, '%10': True}, 'Acc@3': {'@5%': True, '%10': True}}\n",
      "{'P@1': {'@5%': True, '%10': True}, 'Acc@3': {'@5%': False, '%10': False}}\n"
     ]
    }
   ],
   "source": [
    "print(perform_significance_tests(jo_preds['rankingProblemsOutput'], preds_listwise['rankingProblemsOutput']))\n",
    "print(perform_significance_tests(jo_preds['rankingProblemsOutput'], preds_pointwise['rankingProblemsOutput']))\n",
    "print(perform_significance_tests(jo_preds['rankingProblemsOutput'], preds_pairwise['rankingProblemsOutput']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
